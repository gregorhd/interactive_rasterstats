{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solid Waste Quick Assessment Tool\n",
    "\n",
    "## [Jump to Walkthrough](#walkthrough)\n",
    "\n",
    "> Quickly compute, and display on a map, the amount of solid waste generated per week per municipal jurisdiction, and the amount of uncollected solid waste per service area. Applicable to any of the around 30+ countries covered by the [HRSL](https://ciesin.columbia.edu/data/hrsl/#data) at 1 arc-second resolution (2015 data) or globally at 30 arc-second resolution through the [GPW](https://sedac.ciesin.columbia.edu/data/set/gpw-v4-population-count-rev11/data-download) raster (2000, 2005, 2010, 2015 and 2020 data).   \n",
    "\n",
    "This script will compute two statistics (both in metric tonnes): the amount of solid waste generated per week per municipal jurisdiction, and the amount of uncollected solid waste generated in each 'service area', i.e. the area of a municipal jurisdiction for which a service provider (e.g. contractor or municipal department) provides solid waste collection services. \n",
    "\n",
    "Both statistics are presented in a rank-ordered list and as a choropleth map. The example below shows the outputs for Lagos State in Nigeria, using dummy data with respect to service areas.\n",
    "\n",
    "**Results updated in real-time:** A **drop-down menu** allows for the superordinate jurisdiction (e.g. the state-level) to be selected. A **slider** allows for assumptions on the amount of solid waste generated per capita per day (in kilograms) to be adjusted interactively. Both lists and maps are then updated in real-time.\n",
    "\n",
    "![Screen capture of outputs 1 and 2](output1and2.png)\n",
    "![Screen capture of output 3](output3.png)\n",
    "\n",
    "## How it works\n",
    "\n",
    "The script performs simple algebra on the pixel values of a raster layer representing population estimates and adds them as zonal statistics to new attribute fields for two sets of polygon features before plotting each on a map.\n",
    "Each step in the calculation is modularised as a sub-task in a sequence of functions, as pictured below.\n",
    "\n",
    "![Process diagram](process.png)\n",
    "\n",
    "## Installation\n",
    "\n",
    "If you are new to Python, the easiest way to run the script on a specific jurisdiction would be to first install a scientific Python distribution like [Anaconda](https://docs.anaconda.com/anaconda/install/). Anaconda comes with 250 scientific and analytic so-called 'packages' such as NumPy, Pandas, SciPy, Matplotlib, and IPython pre-installed. The Anaconda [Cheat Sheet](https://docs.anaconda.com/_downloads/9ee215ff15fde24bf01791d719084950/Anaconda-Starter-Guide.pdf) offers a good 2-page introduction.\n",
    "\n",
    "Anaconda includes the [conda](https://conda.io/en/latest/) command line interface as well the graphical user interface (GUI) **Navigator**.\n",
    "\n",
    "> Anaconda is available for Windows 7 and newer, macOS 10.10 and newer, or any Linux distribution with a glibc version greater than 2.12 (CentOS 6). It requires 3GB of free hard drive space (Miniconda, a much smaller installer containing only Conda and its dependencies, needs only 400 MB). The environment required for this script (usually to then be found under `C:\\User\\YOURUSERNAME\\anaconda3\\env\\` will take up an additional 1.9GB.\n",
    "\n",
    "The following dependencies are required:\n",
    "\n",
    "```sh\n",
    "  - python=3.8.8\n",
    "  - geopandas=0.9.0\n",
    "  - cartopy=0.18.0\n",
    "  - notebook=6.2.0\n",
    "  - rasterio=1.2.0\n",
    "  - rasterstats=0.14.0\n",
    "  - ipywidgets=7.6.3\n",
    "```\n",
    "\n",
    "To ensure access to these packages and avoid [dependency hell](https://en.wikipedia.org/wiki/Dependency_hell), it is necessary to set up an _environment_, the requirements for which are contained in the `environment.yml` file in the root of this repository (or 'repo').\n",
    "\n",
    "After [forking](https://docs.github.com/en/github/getting-started-with-github/fork-a-repo) this repo or downloading the .zip, open Navigator, click on the **Import** button at the bottom of the **Environments** tab, navigate to the `environment.yml` file in the root folder of your local repo, and click **Import**. Setting up the environment with all its packages and dependencies may take a few minutes.\n",
    "\n",
    "## Running the script\n",
    "\n",
    "#### Required source files\n",
    "\n",
    "The script requires three source files:\n",
    "\n",
    "1. a vector source in EPSG:4326 indicating the administrative boundaries for a superordinate sub-national government tier (e.g. the state level) and a second subordinate tier (e.g. the municipal level):\n",
    " > the present sample script uses the administrative boundaries for Nigeria available at [humdata.org](https://data.humdata.org/dataset/nga-administrative-boundaries);\n",
    "2. another vector source in EPSG:4326 representing service areas including an attribute field or column indicating the total amount of solid waste collected per week by each contractor/service provider in metric tonnes:\n",
    " > the present sample script uses dummy polygons and hypothetical collection totals for Lagos State assuming a 21% collection rate. A second shapefile containing dummy data for Ogun State is available in the `data_files` folder as well. **Switch between Lagos and Ogun in the drop-down menu** to demonstrate how the script can be quickly run on any jurisdiction;       \n",
    "3. the pop GeoTIFF of the CIESIN [High-Resolution Settlements Layer (HRSL)](https://ciesin.columbia.edu/data/hrsl/#data) providing the number of persons estimated to haved lived in each 1 arc-second pixel (roughly 30m) in 2015, available for roughly 30 countries in Africa, Asia and Latin America, **or** the [Gridded World Population](https://sedac.ciesin.columbia.edu/data/collection/gpw-v4) layer which has global coverage, covers 5 year periods from 2000 to 2020 but only has a 30 arc-second resolution (roughly 1km).\n",
    " > the present sample script uses the HRSL for Nigeria available [here](https://ciesin.columbia.edu/data/hrsl/#data).\n",
    "\n",
    "#### Required script adjustments\n",
    "\n",
    "As part of the `main()` function definition, two default values need to be adjusted:\n",
    "\n",
    "1. the `state_list` default value which indicates the state to be selected on-load. Setting this is useful when service area data sources are not available for all states, as in the case of the sample data. Selecting such a state, or having the first state in the list be selected automatically, will cause an `OpenFailedError`;\n",
    "2. the `sw_ppd` _min_, _max_ and _step_ floats indicating the amount of solid waste generated per capita per day in kilograms, according to your particular context (the map annotation will be adjusted automatically).\n",
    "\n",
    "Enclosing scope variables will need to be adjusted in the `USER INPUTS 1` section. These are:\n",
    "\n",
    "3. the `mun_name_field` variable indicating the name of the column containing the names of municipalities;\n",
    "4. the `fp_service_areas` variable indicating the file path to the service areas data sources - the sample data appends '_statename.shp' to 'service_areas' to construct file names;\n",
    "5. the `provider_name_field` variable indicating the name of the column containing the names of service providers;\n",
    "6. the `provider_coll_field` variable indicating the name of the column containing the weekly collection totals reported;\n",
    "7. the `fp_raster` variable indicating the filepath to the raster data source;\n",
    "\n",
    "Global scope variables will need to be adjusted in the `USER INPUTS 2` section, namely:\n",
    "\n",
    "8. the `fp_adm` variable indicating the filepath to the administrative boundaries data source;\n",
    "9. the `state_name_field` variable indicating the name of the column containing the names of the superordinate (e.g. state-level) jurisdictions. If this information is in a data source separate from the subordinate tier, a spatial join via GeoPandas or a desktop GIS may be necessary.\n",
    "\n",
    "#### Optional customization\n",
    "\n",
    "Changing the `stat_select` variable to another statistic supported by _rasterstats_ (such as min, max, mean etc.) is also possible, though then the array algebra and the title of the choropleth maps (via `var_name`) will need to be adjusted accordingly.  \n",
    "\n",
    "#### How to run the script from inside an IDE or text editor\n",
    "\n",
    "To more fluidly edit the script and see the results side-by-side in the same window, you may want to consider using an IDE or code editor with a Python extension. A number of options like [PyCharm](https://www.jetbrains.com/pycharm/) or [Spyder](https://www.spyder-ide.org/) are available, with [Visual Studio Code's Python Extension](https://code.visualstudio.com/docs/languages/python) pictured below. \n",
    "\n",
    "![Screenshot of the Visual Studio Code Python Extension](instructions_02.png)\n",
    "\n",
    "Adding `# %%` to the top of the script turns the script into a [Jupyter-like code cell](https://code.visualstudio.com/docs/python/jupyter-support-py) which allows for a seamless workflow of editing and results visualization in the same window. Simply edit the script on the left and hit `Shift + Enter`to execute and display the results in the interactive interpreter on the right.\n",
    "\n",
    "#### Expected Outputs\n",
    "\n",
    "The script will return three outputs:\n",
    "1. a list of municipalities by amount of solid waste generated per week in metric tonnes, in descending order;\n",
    "2. a list of service providers by amount of uncollected solid waste per week in metric tonnes, in descending order;\n",
    "3. choropleth maps visualizing each list using a scalar colormap (blues for (1) and reds for (2)).\n",
    "\n",
    "A drop-down menu will allow for selecting superordinate jurisdictions whereas a slider will allow for adjusting assumptions on the amount of solid waste generated per capita per day in kilograms.\n",
    "\n",
    "## Release History\n",
    "\n",
    "* 0.1.0\n",
    "    * first release\n",
    "\n",
    "## Meta\n",
    "\n",
    "Gregor Herda – gregorherda at gmail.com\n",
    "\n",
    "The Solid Waste Quick Assessment Tool is licensed under the terms of the GNU General Public License v3.0 and is available for free. See ``LICENSE`` for more information.\n",
    "\n",
    "[https://github.com/gregorhuh](https://github.com/gregorhuh)\n",
    "\n",
    "## How to Contribute\n",
    "\n",
    "1. Fork it (<https://github.com/gregorhuh/UU_egm722_project/fork>)\n",
    "2. Create your feature branch (`git checkout -b feature/fooBar`)\n",
    "3. Commit your changes (`git commit -am 'Add some fooBar'`)\n",
    "4. Push to the branch (`git push origin feature/fooBar`)\n",
    "5. Create a new Pull Request with comprehensive description of changes\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "* @iamdonovan for his assistance in fixing the affine argument\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough<a id=\"walkthrough\"></a>\n",
    "\n",
    "Hit `Shift + Enter` in each of the following code cells to run the code.\n",
    "\n",
    "#### Importing Modules\n",
    "\n",
    "The script first imports necessary modules which are .py files containing Python scripts which can be executed by the Python interpreter directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.feature import ShapelyFeature\n",
    "import rasterio as rio\n",
    "from rasterstats import zonal_stats\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall structure\n",
    "\n",
    "Subsequently the script’s four sub-task functions are defined as indicated by the **def** keyword which denotes a user defined function. The `main()` function constitutes the script’s entry point. As `main()` executes all sub-task functions it is defined last to make it easier to follow what's happening.\n",
    "\n",
    "\n",
    "#### Sub-task function 1: `getVector()`\n",
    "\n",
    "The first sub-task function called by `main()` is `getVector()` which converts the vector sources to geopandas GeoDataFrames whereby a subset of features is selected based on the name of the superordinate jurisdiction dynamically supplied by interact() as the state_list/state_select variable (the service areas are also turned into a GeoDataFrame, though in the sample data all features are inside the study area, so no subsetting is required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVector(fp_adm, fp_service_areas, state_name_field, state_select='Lagos'):\n",
    "    \"\"\"Returns a subset of vector features and the bounding box (study area).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    fp_adm : str\n",
    "        File path for the administrative boundaries vector source.\n",
    "    fp_service_areas : str\n",
    "        File path for the service areas vector source.\n",
    "    state_name_field : str\n",
    "        The name of the column containing names of superordinate jurisdictions.\n",
    "    state_select : str\n",
    "        The name of the superordinate jurisdiction (state) for which to conduct the analysis.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    municipal_filter : GeoDataFrame\n",
    "        GDF of the jurisdictions.\n",
    "    bbox : ndarray\n",
    "        Bounding box of the study area.\n",
    "    service_areas : GeoDataFrame\n",
    "        GDF of the service areas\n",
    "    \"\"\"\n",
    "\n",
    "    # LOAD ADMINISTRATIVE BOUNDARIES VECTOR DATA\n",
    "\n",
    "    # load Nigeria Local Government Area boundaries (Level 2, 'ADM2_EN'), and select only those LGAS within the larger Lagos State administrative boundary (Level 1, 'ADM1_EN')\n",
    "\n",
    "    municipal_all = gpd.read_file(fp_adm)\n",
    "    \n",
    "    municipal_filter = municipal_all[municipal_all[state_name_field] == state_select]\n",
    "    \n",
    "    # Somehow feed to interact() for drop-down: state_list = sorted(municipal_all[state_name_field].unique().tolist())\n",
    "\n",
    "    # DEFINE STUDY AREA BASED ON VECTOR SELECTION\n",
    "\n",
    "    bbox = municipal_filter.total_bounds\n",
    "\n",
    "    # LOAD VECTOR DATA FOR SERVICE AREAS OF SOLID WASTE SERVICE PROVIDERS\n",
    "\n",
    "    # service_areas = gpd.read_file(fp_service_areas).to_crs(crs)\n",
    "    service_areas = gpd.read_file(fp_service_areas)\n",
    "    \n",
    "    return municipal_filter, bbox, service_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-task function 2: `computeArray()`\n",
    "\n",
    "The subset then serves to define the bounding box or study area as an nd array (´bbox´), defined by the total bounds of the selected features.\n",
    "\n",
    "Subsequently, as part of the ´computeArray()´ sub-task function, the GeoTIFF is read using *rasterio*’s `open()` method creating a DatasetReader object called `dataset`. \n",
    "\n",
    "Importantly, this is done as part of a **with** statement which ensures that the GeoTIFF is closed after the nested block which follows the **with** statement exits, regardless of how it exits.\n",
    "`computeArray()` then unpacks  (`*bbox`) the elements in the nd array to serve as arguments to the DatasetReader’s `window()` method to construct a `Window` object. This object then allows for windowed reading of only the relevant part of the `dataset`  into a 2-d numpy array containing the pixel values of population estimates.\n",
    "\n",
    "Simple algebra is then performed on the array to arrive at the weekly weight of solid waste generated in tonnes.\n",
    "Besides assigning `dataset`’s `affine` attribute to the `affine` variable, the `nodata` attribute is also assigned to a variable. When computing zonal statistics, this later ensures that non-zero NoData values do not affect the computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeArray(fp_raster, bbox, sw_ppd):\n",
    "    \"\"\"Returns the nd array, affine and nodata variables required for zonal_stats.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fp_raster : str\n",
    "        File path to the HRSL or GWP raster source.\n",
    "    bbox : nd array\n",
    "        Bounding box of the study area.\n",
    "    sw_ppw : float\n",
    "        The amount of solid waste generated per capita per day.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    array : nd array\n",
    "        Array representing the amount of solid waste produce per pixel per week in metric tonnes.\n",
    "    affine : Affine\n",
    "        Affine transformation for the study area.\n",
    "    nodata : type depends on raster source\n",
    "        The NoData value of the raster source.\n",
    "    \"\"\" \n",
    "    # 1. LOAD HIGH RESOLUTION SETTLEMENTS LAYER\n",
    "\n",
    "    # Continuous floating point raster layer by CIESIN representing number of persons per 30x30m grid cell\n",
    "    # Sample: Nigeria\n",
    "\n",
    "    with rio.open(fp_raster) as dataset:\n",
    "    \n",
    "        # read CRS and no data attributes, create Window object\n",
    "        crs = dataset.crs\n",
    "        nodata = dataset.nodata\n",
    "        window = dataset.window(*bbox)\n",
    "\n",
    "        # CREATE NUMPY ND ARRAYS\n",
    "\n",
    "        # load a subset of the HRSL corresponding to the study area\n",
    "        pop_array = dataset.read(1, window=window)\n",
    "        affine = dataset.window_transform(window)\n",
    "        pop_array[(pop_array < 0)] = np.nan # sets negative NoData values to NaN to enable array algebra\n",
    "\n",
    "        # Calculate tons of solid waste produced per grid cell rson per week\n",
    "\n",
    "        sw_ppd_array = pop_array * sw_ppd # converts population to solid waste per person and day\n",
    "\n",
    "        sw_ppw_array = sw_ppd_array * 7 # converts daily to weekly figures\n",
    "\n",
    "        array = sw_ppw_array / 1000 # converts kilograms to tons per week (TPW)\n",
    "        \n",
    "        return array, affine, nodata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-task function 3: `zonalStats()`\n",
    "\n",
    "The subsequent `zonalStats()` sub-task function wraps the script’s most central function, `getNamesStats()`. This function is called twice (once for jurisdictions and once for service areas), itself wraps the *rasterstats* `zonal_stats()` function and populates two lists, one for feature names and one for features’ zonal statistics. It takes five positional and one keyword argument.\n",
    "\n",
    "The arguments `vector`, `array` and `stat_select` are passed through to the `zonal_stats` function in the *rasterstats* module representing the vector source, the raster source and the statistic to be computed, respectively. The sum of pixel values will be computed by default.\n",
    "\n",
    "Due to the optional keyword argument `geojson_out=True` (default is `False`) `zonal_stats(`) outputs a list of **dictionaries** containing the polygon features’ attributes in addition to a new attribute containing the zonal statistic. As part of `getNamesStats()` the positional arguments `name_field`, `feature_list` and `name_list` are passed in order to query the `properties` dictionary inside the `zonal_stats()` list output, here called `temp`, by way of a **for** loop and extract, for each polygon feature, its name and corresponding zonal statistic, appending it to two empty lists. As `getNamesStats()` is run twice, the use of the `feature_list` and `stat_list` arguments allows for results to be assigned to different lists during multiple runs of the function.\n",
    "\n",
    "The `array` variable then serves as the raster argument for `getNamesStats()`. The `affine` and `nodata` arguments for `zonal_stats()` are supplied by `computeArray()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonalStats(municipal_filter, service_areas, array, affine, nodata, stat_select, mun_name_field, provider_name_field):\n",
    "        \"\"\"Returns one list each of feature names and zonal statistics.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        municipal_filter : GeoDataFrame\n",
    "            GDF of the jurisdictions.\n",
    "        service_areas : GeoDataFrame\n",
    "            GDF of the service areas\n",
    "        array : nd array\n",
    "            Array representing the amount of solid waste produce per pixel per week in metric tonnes.\n",
    "        affine : Affine\n",
    "            Affine transformation for the study area.\n",
    "        nodata : type depends on raster source\n",
    "            The NoData value of the raster source.\n",
    "        stat_select : str\n",
    "            The statistic to be computed (default is 'sum')\n",
    "        mun_name_field : str\n",
    "            The name of the column containing the names of subordinate jurisdictions.\n",
    "        provider_name_field : str\n",
    "            The name of the column containing the names of service providers.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        mun_names : list\n",
    "            List of names of subordinate jurisdictions.\n",
    "        mun_stats : list\n",
    "            List of zonal statistics for subordinate jurisdictions.\n",
    "        provider_names : list\n",
    "            List of names of service providers\n",
    "        provider_stats : list\n",
    "            List of zonal statistics for service providers.\n",
    "        \"\"\"\n",
    "\n",
    "        def getNamesStats(vector, array, name_field, feature_list, stat_list, stat_select='sum'):\n",
    "            \"\"\"Returns one list each of feature names and zonal statistics.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            vector : path to a vector source or geo-like python object\n",
    "                Python object can be a GeoPandas GeoDataFrame.\n",
    "            raster : ndarray\n",
    "                rasterstats alternative arg, 'path to a GDAL raster', not accepted.   \n",
    "            name_field : str\n",
    "                The vector source's column name containing feature names.\n",
    "            feature_list : str\n",
    "                Temp variable name for feature list. Must be unique in script as run consecutively.\n",
    "            stat_list : str\n",
    "                Temp variable name for statistics list. Must be unique in script.\n",
    "            stat_select : str, optional\n",
    "                String value for any statistic supported by zonal_stats (the default is 'sum').\n",
    "\n",
    "            Returns\n",
    "            ----------\n",
    "            feature_list : list\n",
    "                A list in alphabetical order containing the names of polygon features for which zonal stats are computed.\n",
    "            stat_list : list\n",
    "                A list containing the zonal statistics in the same order as \"feature_list\".\n",
    "            \"\"\"\n",
    "\n",
    "            temp = zonal_stats(vector, array, affine=affine, nodata=nodata, stats=stat_select, geojson_out=True)\n",
    "\n",
    "            for feature_dict in temp:\n",
    "                feature_name = feature_dict['properties'][name_field]\n",
    "                feature_stat = feature_dict['properties'][stat_select]\n",
    "                feature_list.append(feature_name)\n",
    "                stat_list.append(feature_stat)\n",
    "\n",
    "        # CALCULATE ZONAL STATS - BASELINE GENERATION PER MUNICIPALITY\n",
    "\n",
    "        # empty lists of all polygon names and zonal stats to be populated by function\n",
    "\n",
    "        mun_names = []\n",
    "        mun_stats = []\n",
    "        getNamesStats(municipal_filter, array, mun_name_field, mun_names, mun_stats)\n",
    "\n",
    "        # CALCULATE ZONAL STATS - SOLID WASTE COLLECTED PER SERVICE AREA\n",
    "\n",
    "        provider_names = []\n",
    "        provider_stats = []\n",
    "        getNamesStats(service_areas, array, provider_name_field, provider_names, provider_stats)\n",
    "\n",
    "        return mun_names, mun_stats, provider_names, provider_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-task function 4: `processStats()`\n",
    "\n",
    "As the statistic for service areas is the total amount of *un*collected waste, and total waste volume actually collected is provided as part of the `service_area` GeoDataFrame, as part of a final sub-task function `processStats()`, the latter values are first extracted to a **list**\n",
    "\n",
    "```\n",
    "provider_coll = service_areas[provider_coll_field].values.tolist()\n",
    "```\n",
    "which is subsequently turned into a `zip` object (an iterator aggregating the two iterables)\n",
    "\n",
    "```\n",
    "zip_object = zip(provider_stats, provider_coll)\n",
    "```\n",
    "\n",
    "which allows for the actual collected waste volume to be subtracted from the amount estimated to have been generated \n",
    "through a **for** loop and appended to a new **list**.\n",
    "\n",
    "```\n",
    "provider_uncoll = []\n",
    "    for i, j in zip_object:\n",
    "        provider_uncoll.append(i - j)\n",
    "```\n",
    "\n",
    "To print the results as a rank ordered **list**, the two list pairs are zipped and turned into a **dict**\n",
    "\n",
    "```\n",
    "mun_dict = dict(zip(mun_names, mun_stats))\n",
    "provider_dict = dict(zip(provider_names, provider_uncoll))\n",
    "```\n",
    "which allows for the **dict** to be sorted by values through a `lambda` function inside a `sorted()` function.\n",
    "\n",
    "```\n",
    "mun_dict_sorted = sorted(mun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "provider_dict_sorted = sorted(provider_dict.items(), key=lambda x: x[1], reverse=True) \n",
    "```\n",
    "resulting in a **list** containing the rank ordered element pairs which can be printed successively through a **for** loop:\n",
    "\n",
    "```\n",
    "print('Municipalities by solid waste generated per week (descending):\\n')\n",
    "    for i in mun_dict_sorted:\n",
    "        print(i[0],':',f\"{int(i[1]):,}\", 'tonnes')\n",
    "\n",
    "print('\\nService providers by total uncollected solid waste per week (descending)\\n')\n",
    "    for i in provider_dict_sorted:\n",
    "        print(i[0],':',f\"{int(i[1]):,}\", 'tonnes')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processStats(service_areas, mun_names, mun_stats, provider_names, provider_stats, provider_coll_field):\n",
    "        \"\"\"Prints two rank-ordered lists and returns two lists of ordered tuples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        service_areas : GeoDataFrame\n",
    "            GDF of the service areas\n",
    "        mun_names : list\n",
    "            List of names of subordinate jurisdictions.\n",
    "        mun_stats : list\n",
    "            List of zonal statistics for subordinate jurisdictions.\n",
    "        provider_names : list\n",
    "            List of names of service providers\n",
    "        provider_stats : list\n",
    "            List of zonal statistics for service providers.\n",
    "        provider_coll_field : str\n",
    "            Name of the column containing the recorded collection totals for each service provider.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        mun_dict_sorted : list\n",
    "            List of name-stat tuples for subordinate jurisdictions.\n",
    "        provider_dict_sorted : list\n",
    "            List of name-stat tuples for service providers.\n",
    "        \"\"\"\n",
    "\n",
    "        # extract total collection values and subtract the total waste generated in each service area\n",
    "\n",
    "        provider_coll = service_areas[provider_coll_field].values.tolist()\n",
    "        zip_object = zip(provider_stats, provider_coll)\n",
    "\n",
    "        provider_uncoll = []\n",
    "        for i, j in zip_object:\n",
    "            provider_uncoll.append(i - j)\n",
    "\n",
    "        # ORGANISE AND PRINT RESULTS\n",
    "\n",
    "        # combine populated lists into dictionaries\n",
    "        mun_dict = dict(zip(mun_names, mun_stats))\n",
    "        provider_dict = dict(zip(provider_names, provider_uncoll))\n",
    "\n",
    "        # sort dictionaries by value in descending order into list of tuples\n",
    "        mun_dict_sorted = sorted(mun_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "        provider_dict_sorted = sorted(provider_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # print the items in the sorted list of tuples\n",
    "        print('Municipalities by solid waste generated per week (descending):\\n')\n",
    "        for i in mun_dict_sorted:\n",
    "            print(i[0],':',f\"{int(i[1]):,}\", 'tonnes')\n",
    "\n",
    "        print('\\nService providers by total uncollected solid waste per week (descending)\\n')\n",
    "        for i in provider_dict_sorted:\n",
    "            print(i[0],':',f\"{int(i[1]):,}\", 'tonnes')\n",
    "\n",
    "        return mun_dict_sorted, provider_dict_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `main()` function\n",
    "\n",
    "`main()` contains the **USER INPUTS 1** section. Here all hard coded variables with enclosing scope for which user input is required are listed for easy of access. Variables relate to file paths for service areas and the raster source, attribute names specific to the respective data source, and optional variables for the zonal statistic to be computed along with a variable adjusting the map title accordingly. As `main()` also executes all subsequent functions, this makes these variables accessible to subsequently called functions.\n",
    "\n",
    "Before the results can be plotted on a map, they must be added back to the respective GeoDataFrames as a new column, which is done by `main()`. Two different approaches fulfilling the same purpose are used: the GeoDataFrame’s `assign()` method and a **for** loop. In the former, a `stat_output` column is created, in the latter a `total_uncoll` column. \n",
    "\n",
    "```\n",
    "municipal_filter = municipal_filter.assign(\n",
    "        stat_output = pd.Series(mun_stats, index = municipal_filter.index)\n",
    "    )\n",
    "\n",
    "for i, row in service_areas.iterrows():\n",
    "        service_areas.loc[i, 'total_uncoll'] = provider_stats[i] - row[provider_coll_field]\n",
    "```\n",
    "According to the values in these columns, features are then plotted by *matplotlib* \n",
    "\n",
    "```\n",
    "municipal_filter.plot(column='stat_output', cmap='Blues', linewidth=0.8, ax=ax1, edgecolor='0.8')\n",
    "service_areas.plot(column='total_uncoll', cmap='Reds', linewidth=0.8, ax=ax2, edgecolor='k')\n",
    "```\n",
    "onto two separate subplots (ax1, ax2)\n",
    "\n",
    "```\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 7.5), subplot_kw=dict(projection=myCRS))\n",
    "```\n",
    "\n",
    "The respective colorbars are normalized through their minimum and maximum values\n",
    "\n",
    "```\n",
    "vmin, vmax =  municipal_filter['stat_output'].min(), municipal_filter['stat_output'].max()\n",
    "vmin2, vmax2 =  service_areas['total_uncoll'].min(), service_areas['total_uncoll'].max()\n",
    "```\n",
    "\n",
    "assigning each a separate color map\n",
    "\n",
    "```\n",
    "sm = plt.cm.ScalarMappable(cmap='Blues', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "fig.colorbar(sm, ax=ax1, orientation=\"horizontal\")\n",
    "\n",
    "sm.set_array([])\n",
    "fig.colorbar(sm, ax=ax2, orientation=\"horizontal\")\n",
    "```\n",
    "\n",
    "Certain keywords of both the map title and annotation are linked to the user inputs section.\n",
    "\n",
    "```\n",
    "title = var_name + ' by municipalities in ' + state_select + ' State'\n",
    "ax1.set_title(title, fontdict={'fontsize': '12', 'fontweight' : '5'})\n",
    "    \n",
    "ax1.annotate('Source: CIESIN HRSL, assuming ' + str(sw_ppd) + 'kg of solid waste per capita per day', xy=(0.225, .025), xycoords='figure fraction', fontsize=12, color='#555555')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(state_list='Lagos', sw_ppd=(0.4, 1.2, 0.1)): \n",
    "    \"\"\"Executes all functions, adds zonal stats to GDFs and plots results on a subplot each.\n",
    "\n",
    "    The majority of hard coded user inputs are declared as enclosing scope variables here.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state_list : str or list\n",
    "        Setting the default string value in the function definition causes this state to be active on-load. This is useful when service area source files are not available for all states (selecting such a state will cause and OpenFailedError). When interact() is executed, the list of states is passed to main().\n",
    "    sw_ppd : tuple\n",
    "        Floats for min, max and step controlling the ipywidgets slider\n",
    "    \"\"\"\n",
    "\n",
    "    # USER INPUT 1: Enclosing Scope Variables\n",
    "\n",
    "    # equates the element from the list passed to main() by interact() with state_select\n",
    "    state_select = state_list\n",
    "\n",
    "    # indicate the name of the column containing the names of municipalities\n",
    "    mun_name_field = 'ADM2_EN'\n",
    "\n",
    "    # indicating the filepath to the service areas data source(s)\n",
    "    fp_service_areas = 'data_files/01_input/01_vector/service_areas_' + state_select.lower() + '.shp'\n",
    "\n",
    "    # indicating the name of the column containing the names of service providers;\n",
    "    provider_name_field = 'psp_name'\n",
    "\n",
    "    # indicate the name of the column containing the weekly collection totals reported\n",
    "    provider_coll_field = 'total_coll'\n",
    "\n",
    "    # indicate the filepath to the raster data source;\n",
    "    fp_raster = 'data_files/01_input/02_raster/hrsl_nga_pop.tif'\n",
    "\n",
    "    # OPTIONAL CUSTOMIZATIONS\n",
    "\n",
    "    # the rasterstats zonal statistic to be computed for each jurisdiction (default is 'sum')\n",
    "    stat_select = 'sum' \n",
    "\n",
    "    # If adjusting 'stat_select', also adjust the title of the choropleth maps accordingly\n",
    "    var_name = 'Tonnes of solid waste generated per week'\n",
    "    \n",
    "    # EXECUTE FUNCTIONS\n",
    "\n",
    "    municipal_filter, bbox, service_areas = getVector(fp_adm, fp_service_areas, state_name_field, state_select)\n",
    "\n",
    "    array, affine, nodata = computeArray(fp_raster, bbox, sw_ppd)\n",
    "\n",
    "    mun_names, mun_stats, provider_names, provider_stats = zonalStats(municipal_filter, service_areas, array, affine, nodata, stat_select, mun_name_field, provider_name_field)\n",
    "\n",
    "    mun_dict_sorted, provider_dict_sorted = processStats(service_areas, mun_names, mun_stats, provider_names, provider_stats, provider_coll_field)\n",
    "\n",
    "    # UPDATE GEODATAFRAMES WITH RESULTS\n",
    "    \n",
    "    # Assign zonal statistics to new columns \n",
    "    # 'stat_output' for municipal_filter GDF and 'total_uncoll' for service_areas GDF\n",
    "    # using two different methods\n",
    "    \n",
    "    municipal_filter = municipal_filter.assign(\n",
    "            stat_output = pd.Series(mun_stats, index = municipal_filter.index)\n",
    "    )\n",
    "\n",
    "    for i, row in service_areas.iterrows():\n",
    "            service_areas.loc[i, 'total_uncoll'] = provider_stats[i] - row[provider_coll_field]\n",
    "    \n",
    "    # PLOT RESULTS ON CHOROPLETH MAPS\n",
    "    \n",
    "    # Define figure CRS and canvas layout\n",
    "\n",
    "    myCRS = ccrs.Mercator()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 7.5), subplot_kw=dict(projection=myCRS))\n",
    "\n",
    "    # --Subplot 1-- \n",
    "    \n",
    "    # add municipal boundaries\n",
    "\n",
    "    municipal_feat = ShapelyFeature(municipal_filter['geometry'], myCRS, facecolor='none', edgecolor='k', linewidth=0.5)\n",
    "    ax1.add_feature(municipal_feat)\n",
    "\n",
    "    # add dynamic title and annotation\n",
    "\n",
    "    title = var_name + ' by municipalities in ' + state_select + ' State'\n",
    "    ax1.set_title(title, fontdict={'fontsize': '12', 'fontweight' : '5'})\n",
    "\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax1.annotate('Source: CIESIN HRSL, assuming ' + str(sw_ppd) + 'kg of solid waste per capita per day', xy=(0.225, .025), xycoords='figure fraction', fontsize=12, color='#555555')\n",
    "\n",
    "    # create colorbar legend\n",
    "\n",
    "    vmin, vmax =  municipal_filter['stat_output'].min(), municipal_filter['stat_output'].max()\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap='Blues', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "    sm.set_array([])\n",
    "\n",
    "    fig.colorbar(sm, ax=ax1, orientation=\"horizontal\")\n",
    "\n",
    "    municipal_filter.plot(column='stat_output', cmap='Blues', linewidth=0.8, ax=ax1, edgecolor='0.8')\n",
    "\n",
    "    # --Sub-plot 2--\n",
    "\n",
    "    # add dynamic title and annotation\n",
    "\n",
    "    title2 = 'Tonnes of uncollected solid waste per week by service area (dummy data)'\n",
    "    ax2.set_title(title2, fontdict={'fontsize': '12', 'fontweight' : '5'})\n",
    "\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Create colorbar legend\n",
    "\n",
    "    vmin2, vmax2 =  service_areas['total_uncoll'].min(), service_areas['total_uncoll'].max()\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=vmin2, vmax=vmax2))\n",
    "\n",
    "    sm.set_array([])\n",
    "\n",
    "    fig.colorbar(sm, ax=ax2, orientation=\"horizontal\")\n",
    "\n",
    "    municipal_filter.plot(facecolor='none', linewidth=0.5, ax=ax2, edgecolor='k')\n",
    "\n",
    "    service_areas.plot(column='total_uncoll', cmap='Reds', linewidth=0.8, ax=ax2, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing `main()` through `interact()`\n",
    "\n",
    "Global scope variables are defined in the **USER INPUTS 2** section outside any function. This makes state_list accessible to `interact()` allowing the drop-down menu to be generated.\n",
    "\n",
    "`main()` is called by the *ipywidget* `interact()` function to interactively manipulate the `state_list` variable for selecting study areas (passed to `getVector()` and the `sw_ppd` variable indicating the amount of solid waste generated per capita per day which is passed to the `computeArray()` sub-task function whenever the user adjusts the slider widget. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8c1f6567c049dfa84e8b1362b74fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='state_list', index=24, options=('Abia', 'Adamawa', 'Akwa Ibom', 'A…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.main(state_list='Lagos', sw_ppd=(0.4, 1.2, 0.1))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USER INPUTS 2: Global Scope Variables\n",
    "# this set of variables is declared globally to make state_list accessible to interact()\n",
    "\n",
    "fp_adm = 'data_files/01_input/01_vector/nga_admbnda_adm2_osgof_20190417.shp'\n",
    "\n",
    "# indicate the name of the column indicating the names of the superordinate (e.g. state-level) jurisdictions\n",
    "# If this information is in a data source separate from the subordinate tier,\n",
    "# a spatial join via GeoPandas or a desktop GIS may be necessary\n",
    "\n",
    "state_name_field = 'ADM1_EN'\n",
    "\n",
    "# variables declared to be passed to interact()\n",
    "\n",
    "municipal_all = gpd.read_file(fp_adm)\n",
    "\n",
    "state_list = sorted(municipal_all[state_name_field].unique().tolist())\n",
    "\n",
    "# EXECUTE\n",
    "\n",
    "interact(main, state_list=state_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
